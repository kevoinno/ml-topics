{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling data \n",
    "\n",
    "## Effects of Scaling\n",
    "\n",
    "### Scaling affects interpretability\n",
    "Let's say we have following ordinary least squares regressions.\n",
    "\n",
    "1. $y = \\hat \\beta_0 + \\hat \\beta_1x_1 + \\hat \\beta_2x_2$\n",
    "\n",
    "Where $x_1$ and $x_2$ are in meters.\n",
    "\n",
    "2. $y = \\hat \\beta_0 + \\hat \\beta_1x_1 + \\hat \\beta_2x_2$\n",
    "\n",
    "Where $x_1$ is in meters and $x_2$ is in centimeters.\n",
    "\n",
    "For 1., we can say that the larger $\\beta$ has a greater effect on y, because $x_1$ and $x_2$ are on the same scale.\n",
    "\n",
    "For 2., the $\\hat \\beta_2$ is 100 times smaller than the same coefficient in 1. So we can't conclude that $\\beta_1$ is a more important variable than $\\beta_2$, because the scales are different.\n",
    "\n",
    "### When to scale or not scale?\n",
    "\n",
    "**Scale** when your machine learning model includes distances (e.g. SVM, KNN, KMeans, Linear Regression) or uses gradient descent because it can help with convergence. We scale when distance calculations are involved because features with large scales can overpower the distance calculation.\n",
    "![](knn_scaling.JPG)\n",
    "\n",
    "Don't need to scale when distances aren't involved (e.g. trees). Trees are unaffected because they will split in the same relative position whether the data is scaled or not.\n",
    "\n",
    "**In general, it does not hurt to scale the data.**\n",
    "\n",
    "### Types of Scaling\n",
    "\n",
    "#### Standardization vs Normalization\n",
    "\n",
    "**Normalization (Min-max scaler)** gets our feature values between 0 and 1.\n",
    "\n",
    "$X_{norm} = \\frac{X-X_{min}}{X_{max} - X_{min}}$\n",
    "\n",
    "We use normalization when we use CNNs.\n",
    "\n",
    "**Standardization (Standard scaler)** gets our features into z-scores\n",
    "\n",
    "$Z = \\frac{X - \\mu}{\\sigma}$\n",
    "\n",
    "Usually, standardization performs better than normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do an example to see how scaling affects our regression. In this example, we are trying to predict the mpg of a car based on its engine size, horsepower, and weight. The data was simulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate the toy dataset\n",
    "n_samples = 100\n",
    "\n",
    "# Features: engine_size (in liters), horsepower, weight (in kg)\n",
    "engine_size = np.random.uniform(1.0, 4.0, n_samples)  # Engine size between 1.0 and 4.0 liters\n",
    "horsepower = engine_size * np.random.uniform(60, 100, n_samples)  # Roughly proportional to engine size\n",
    "weight = np.random.uniform(800, 2000, n_samples)  # Weight between 800 and 2000 kg\n",
    "\n",
    "# Target variable: mpg (miles per gallon), inversely related to horsepower and weight\n",
    "mpg = 50 - (horsepower / 50) - (weight / 1000) + np.random.normal(0, 2, n_samples)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'engine_size': engine_size,\n",
    "    'horsepower': horsepower,\n",
    "    'weight': weight,\n",
    "    'mpg': mpg\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's split the data into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('mpg', axis = 1)\n",
    "y = df['mpg']\n",
    "\n",
    "# Split into 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=np.random.seed(42))\n",
    "\n",
    "# Scale features using Min-Max (Normalization)\n",
    "mm_scaler = MinMaxScaler()\n",
    "X_train_mm = mm_scaler.fit_transform(X_train)\n",
    "X_test_mm = mm_scaler.transform(X_test)\n",
    "\n",
    "# Scale features using Standard scaler (Standardization)\n",
    "ss_scaler = StandardScaler()\n",
    "X_train_ss = mm_scaler.fit_transform(X_train)\n",
    "X_test_ss = mm_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets perform a linear regression unscaled, normalized, and standardized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of unscaled data: 8.409350248370767\n",
      "MSE of Min-Max data: 8.409350248370782\n",
      "MSE of standardized data: 8.409350248370782\n"
     ]
    }
   ],
   "source": [
    "# Unscaled\n",
    "lm_unscaled = LinearRegression()\n",
    "lm_unscaled.fit(X_train, y_train)\n",
    "pred_unscaled = lm_unscaled.predict(X_test)\n",
    "print(f\"MSE of unscaled data: {mean_squared_error(y_test, pred_unscaled)}\")\n",
    "\n",
    "# Min-Max (Normalization)\n",
    "lm_mm = LinearRegression()\n",
    "lm_mm.fit(X_train_mm, y_train)\n",
    "pred_mm = lm_mm.predict(X_test_mm)\n",
    "print(f\"MSE of Min-Max data: {mean_squared_error(y_test, pred_mm)}\")\n",
    "\n",
    "\n",
    "lm_ss = LinearRegression()\n",
    "lm_ss.fit(X_train_ss, y_train)\n",
    "pred_ss = lm_ss.predict(X_test_ss)\n",
    "print(f\"MSE of standardized data: {mean_squared_error(y_test, pred_ss)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It look like scaling made a huge difference here in terms of performance. Let's see what happens when we use KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of unscaled data: 8.409350248370767\n",
      "MSE of normalized data: 8.366496434906873\n",
      "MSE of standardized data: 8.366496434906873\n"
     ]
    }
   ],
   "source": [
    "# Unscaled\n",
    "knn_unscaled = KNeighborsRegressor()\n",
    "knn_unscaled.fit(X_train, y_train)\n",
    "pred_unscaled = lm_unscaled.predict(X_test)\n",
    "print(f\"MSE of unscaled data: {mean_squared_error(y_test, pred_unscaled)}\")\n",
    "\n",
    "# Min-Max (Normalization)\n",
    "knn_mm = KNeighborsRegressor()\n",
    "knn_mm.fit(X_train_mm, y_train)\n",
    "pred_mm = knn_mm.predict(X_test_mm)\n",
    "print(f\"MSE of normalized data: {mean_squared_error(y_test, pred_mm)}\")\n",
    "\n",
    "# Standard Scaler (standardization)\n",
    "knn_ss = KNeighborsRegressor()\n",
    "knn_ss.fit(X_train_ss, y_train)\n",
    "pred_ss = knn_ss.predict(X_test_ss)\n",
    "print(f\"MSE of standardized data: {mean_squared_error(y_test, pred_ss)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is a slight improvement in MSE when we scale using standardization or normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that trees based models are not really affected by scaling, so we will show this property now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of unscaled data: 11.372798254414686\n",
      "MSE of unscaled data: 11.372798254414686\n",
      "MSE of unscaled data: 11.372798254414686\n"
     ]
    }
   ],
   "source": [
    "tree_unscaled = DecisionTreeRegressor(random_state=np.random.seed(42))\n",
    "tree_unscaled.fit(X_train, y_train)\n",
    "pred_unscaled = tree_unscaled.predict(X_test)\n",
    "print(f\"MSE of unscaled data: {mean_squared_error(y_test, pred_unscaled)}\")\n",
    "\n",
    "tree_mm = DecisionTreeRegressor(random_state=np.random.seed(42))\n",
    "tree_mm.fit(X_train_mm, y_train)\n",
    "pred_mm = tree_mm.predict(X_test_mm)\n",
    "print(f\"MSE of unscaled data: {mean_squared_error(y_test, pred_mm)}\")\n",
    "\n",
    "tree_ss = DecisionTreeRegressor(random_state=np.random.seed(42))\n",
    "tree_ss.fit(X_train_ss, y_train)\n",
    "pred_ss = tree_ss.predict(X_test_ss)\n",
    "print(f\"MSE of unscaled data: {mean_squared_error(y_test, pred_ss)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the MSE of the decision tree models do not change even if we scale the features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
